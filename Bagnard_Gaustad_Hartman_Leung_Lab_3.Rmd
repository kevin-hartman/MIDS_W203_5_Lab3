---
title: 'Local Policy Recommendations for Crime Reduction'
subtitle: 'Final Report'
author: 
- "Alexa Bagnard, Joseph Gaustad, Kevin Hartman, Francis Leung"
- "(W203 Wednesday 6:30pm Summer 2019)"
date: "8/7/2019"
tags: [crime, lab, w203, berkeley]
abstract: This is our study on crime. Crime does not pay. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
output:
  pdf_document: 
    latex_engine: xelatex
    fig_caption: yes
    fig_crop: no
    highlight: haddock
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    fig_caption: yes
    theme: journal
    toc: yes
    toc_depth: 2
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
library(stargazer)
library(cowplot)
library(ggplot2)
library(tidyr)
library(dplyr)
library(reshape2)
options(kableExtra.latex.load_packages = TRUE)
library(kableExtra)
library(plyr)
library(expss)
library(car)
library(corrplot)
library(gridExtra)
library(corrr)
library(Hmisc)
library(broom)
library(lmtest)
library(sandwich)
library(PerformanceAnalytics)
library(psych)
library(GGally)
library(ggcorrplot)
library(tmap)
library(sf)

suppressMessages(library(stargazer))
suppressMessages(library(cowplot))
suppressMessages(library(ggplot2))
suppressMessages(library(tidyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(kableExtra))
suppressMessages(library(plyr))
suppressMessages(library(expss))
suppressMessages(library(car))
suppressMessages(library(corrplot))
suppressMessages(library(gridExtra))
suppressMessages(library(corrr))
suppressMessages(library(Hmisc))
suppressMessages(library(broom))
suppressMessages(library(lmtest))
suppressMessages(library(sandwich))
suppressMessages(library(PerformanceAnalytics))
suppressMessages(library(psych))
suppressMessages(library(GGally))
suppressMessages(library(ggcorrplot))
suppressMessages(library(knitr))
suppressMessages(library(tmap))
suppressMessages(library(sf))
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction


## Background

In this report, we seek to examine and discuss determinants of crime and offer recommend actionable policy recommendations for local politicians running for election at the county level in North Carolina. For our analysis, we draw on sample data collected from a study by Cornwell and Trumball, researchers from the University of Georgia and West Virginia University. Our sample data includes data on crime rates, arrests, sentences,  demographics, local weekly wages, tax revenues and more drawn from local and federal government data sources. Although the age of the data may be a potential limitation of our study, we believe the insights we gather and policy recommendations remain appropriate for local campaigns today.

Our primary question that will drive our data exploration are to ask which variables affect crime rate the most.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=FALSE}
#Add a nice little map of North Carolina and shade it by region.

nc <- read_sf(system.file("shape/nc.shp", package = "sf"))
nc$REGION<-"COASTAL/OTHER"
nc$REGION[nc$NAME=="Ashe"]<-"WEST"
nc$REGION[nc$NAME=="Alleghany"]<-"WEST"
nc$REGION[nc$NAME=="Wilkes"]<-"WEST"
nc$REGION[nc$NAME=="Watauga"]<-"WEST"
nc$REGION[nc$NAME=="Caldwell"]<-"WEST"
nc$REGION[nc$NAME=="Avery"]<-"WEST"
nc$REGION[nc$NAME=="Burke"]<-"WEST"
nc$REGION[nc$NAME=="Rutherford"]<-"WEST"
nc$REGION[nc$NAME=="Polk"]<-"WEST"
nc$REGION[nc$NAME=="Mitchell"]<-"WEST"
nc$REGION[nc$NAME=="McDowell"]<-"WEST"
nc$REGION[nc$NAME=="Yancey"]<-"WEST"
nc$REGION[nc$NAME=="Madison"]<-"WEST"
nc$REGION[nc$NAME=="Buncombe"]<-"WEST"
nc$REGION[nc$NAME=="Henderson"]<-"WEST"
nc$REGION[nc$NAME=="Transylvania"]<-"WEST"
nc$REGION[nc$NAME=="Haywood"]<-"WEST"
nc$REGION[nc$NAME=="Jackson"]<-"WEST"
nc$REGION[nc$NAME=="Swain"]<-"WEST"
nc$REGION[nc$NAME=="Macon"]<-"WEST"
nc$REGION[nc$NAME=="Graham"]<-"WEST"
nc$REGION[nc$NAME=="Cherokee"]<-"WEST"
nc$REGION[nc$NAME=="Clay"]<-"WEST"
nc$REGION[nc$NAME=="Surry"]<-"CENTRAL"
nc$REGION[nc$NAME=="Yadkin"]<-"CENTRAL"
nc$REGION[nc$NAME=="Alexander"]<-"CENTRAL"
nc$REGION[nc$NAME=="Catawba"]<-"CENTRAL"
nc$REGION[nc$NAME=="Lincoln"]<-"CENTRAL"
nc$REGION[nc$NAME=="Gaston"]<-"CENTRAL"
nc$REGION[nc$NAME=="Cleveland"]<-"CENTRAL"
nc$REGION[nc$NAME=="Stokes"]<-"CENTRAL"
nc$REGION[nc$NAME=="Forsyth"]<-"CENTRAL"
nc$REGION[nc$NAME=="Davie"]<-"CENTRAL"
nc$REGION[nc$NAME=="Rowan"]<-"CENTRAL"
nc$REGION[nc$NAME=="Cabarrus"]<-"CENTRAL"
nc$REGION[nc$NAME=="Mecklenburg"]<-"CENTRAL"
nc$REGION[nc$NAME=="Union"]<-"CENTRAL"
nc$REGION[nc$NAME=="Stanly"]<-"CENTRAL"
nc$REGION[nc$NAME=="Davidson"]<-"CENTRAL"
nc$REGION[nc$NAME=="Rockingham"]<-"CENTRAL"
nc$REGION[nc$NAME=="Guilford"]<-"CENTRAL"
nc$REGION[nc$NAME=="Randolph"]<-"CENTRAL"
nc$REGION[nc$NAME=="Montgomery"]<-"CENTRAL"
nc$REGION[nc$NAME=="Anson"]<-"CENTRAL"
nc$REGION[nc$NAME=="Richmond"]<-"CENTRAL"
nc$REGION[nc$NAME=="Caswell"]<-"CENTRAL"
nc$REGION[nc$NAME=="Alamance"]<-"CENTRAL"
nc$REGION[nc$NAME=="Chatham"]<-"CENTRAL"
nc$REGION[nc$NAME=="Moore"]<-"CENTRAL"
nc$REGION[nc$NAME=="Lee"]<-"CENTRAL"
nc$REGION[nc$NAME=="Person"]<-"CENTRAL"
nc$REGION[nc$NAME=="Orange"]<-"CENTRAL"
nc$REGION[nc$NAME=="Durham"]<-"CENTRAL"
nc$REGION[nc$NAME=="Granville"]<-"CENTRAL"
nc$REGION[nc$NAME=="Wake"]<-"CENTRAL"
nc$REGION[nc$NAME=="Vance"]<-"CENTRAL"
nc$REGION[nc$NAME=="Franklin"]<-"CENTRAL"
nc$REGION[nc$NAME=="Warren"]<-"CENTRAL"
nc$REGION[nc$NAME=="Iredell"]<-"CENTRAL"
ggplot() + 
  geom_sf(data = nc, aes(fill = REGION)) + theme_map() + 
  scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))
```

## The Variables

The crime_v2 dataset provided includes 25 variables of interest. 

We include them below for reference by category of interest.

\begin{center}
\textbf{Data Dictionary}
\end{center}
Category | Variable
---------- | -------------------------------
Crime Rate | crmrte
Geographic | county, west, central
Demographic | urban, density, pctmin80, pctymle
Economic - Wage | wcon, wtuc, wtrd, wfir, wser, wmfg, wfed, wsta, wloc
Economic - Revenue  | taxpc
Law Enforcment | polpc, prbarr, prbconv, mix
Judicial/Sentencing | prbpris, avgsen
Time Period | year
\begin{center}
Table 1: Data Dictionary
\end{center}

The variables above operationalize the conditions we wish to explore and their affects on crime rate

Chiefly, these break down as follows.

* The Economic variables measures the county's economic activity and health (e.g. opportunity to pursue legal forms of income). These variables come in the form of available wages and tax revenue returned to the county.

* The Law enforcment variables measures the county's ability to utilize law enforcment policy to deter crime.  Similarly, the Judicial variables also signify impact of deterence to crime.

* The Demographic variables measure the cultural variability that represent the social differences between each county, such as urban vs rural and minority populations.

* The Geographic elements are categorical. They  represent they ways in which the population is segmented by geography.


# Exploratory Data Analysis (EDA)

## Data Prep and Exploration

We begin our analysis by loading the data set and performing basic checks and inspections.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime = read.csv("crime_v2.csv")
summary(dfCrime)
```

First, we will remove the missing rows from the dataset.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
nrow(dfCrime)
dfCrime <-na.omit(dfCrime) # omit the NA rows
nrow(dfCrime)
```

Next, we will inspect the data to see if there are duplicate records


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime[duplicated(dfCrime),]
```

A duplicate row exists. We'll remove it.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime <- dfCrime[!duplicated(dfCrime),] # remove the duplicated row
```

We also saw that pbconv was coded as a level. It is not a level but a ratio. We'll change that now.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$prbconv<-as.numeric(levels(dfCrime$prbconv))[dfCrime$prbconv]
```

We also notice by comparision of pctymle and pctmin80 one of the variables is off by a factor of 100. We will divide pctmin80 by 100 so the two variables are in the same unit terms.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$pctmin80<-dfCrime$pctmin80/100
```

County was expressed as a number. However, it is a categorical variable and we will convert it to a factor instead.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$county<-as.factor(dfCrime$county)
```

Next we inspect the indicator variables to see if they were coded correctly.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime %>% group_by(west, central) %>% tally()
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime %>%
filter(west ==1 & central ==1)
```

One county was either mis-coded (with west=1 and central=1), or it truly belongs to both regions. However, this is very unlikely as the proper coding technique is to widen the data and introduce indicator variables for each category. It is not likley that data was captured for both categories.

We will need further analysis on this datapoint to assess proper treatment options.

For now, we will encode a new region variable and place the datapoint in its own category. 


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#Map central and west to a region code, and create a new category for other
# Note that county 71 has both western and central codes
dfCrime$region <- case_when (
            (dfCrime$central ==0 & dfCrime$west ==0) ~ 0, #Eastern, Coastal, Other
            (dfCrime$central ==0 & dfCrime$west ==1) ~ 1, #Western
            (dfCrime$central ==1 & dfCrime$west ==0) ~ 2, #Central
            (dfCrime$central ==1 & dfCrime$west ==1) ~ 3 #Central-Western county?
        )
dfCrime$regcode =
            factor( dfCrime$region , levels = 0:3 , labels =
                    c( 'Other',
                       'West',
                       'Central',
                       'CW')
                   )
```

We will also introduce an indicator variable for counties located in the "other" region that are not west or central


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$other <- ifelse((dfCrime$central ==0 & dfCrime$west ==0), 1, 0)
```

And we'll add an indicator variable to serve as complement to the urban indicator variable and call this 'nonurban'


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$nonurban <- ifelse((dfCrime$urban==0), 1, 0)
```

By way of the 1980 Census fact sheet, we discover the urban field is an encoding for SMSA (Standard Metropolitan Statistical Areas). https://www2.census.gov/prod2/decennial/documents/1980/1980censusofpopu8011uns_bw.pdf
The value is one if the county is inside a metropolitan area. Otherwise, if the county is outside a metropolitan area, the value is zero.

We create a metro factor variable to better describe this feature.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
# create factor for SMSA (standard metropolitan statistical areas) with two levels 
# (inside or outside)
#    https://www2.census.gov/prod2/decennial/documents/1980/1980censusofpopu8011uns_bw.pdf
dfCrime$metro =
            factor( dfCrime$urban , levels = 0:1 , labels =
                    c( 'Outside Metro',
                       'Inside Metro'
                      )
                   )
```

Next we will visualize each variable through boxplots and subdvide the datapoints by region.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#Plot of the economic and tax related variables vs crmrte
q1<-ggplot(data = dfCrime, aes(y = wcon, color = regcode)) + 
      geom_boxplot()
q2<-ggplot(data = dfCrime, aes(y = wtuc, color = regcode)) + 
      geom_boxplot()
q3<-ggplot(data = dfCrime, aes(y = wtrd, color = regcode)) + 
      geom_boxplot()
q4<-ggplot(data = dfCrime, aes(y = wfir, color = regcode)) + 
      geom_boxplot()
q5<-ggplot(data = dfCrime, aes(y = wser, color = regcode)) + 
      geom_boxplot()
q6<-ggplot(data = dfCrime, aes(y = wmfg, color = regcode)) + 
      geom_boxplot()
q7<-ggplot(data = dfCrime, aes(y = wfed, color = regcode)) + 
      geom_boxplot()
q8<-ggplot(data = dfCrime, aes(y = wsta, color = regcode)) + 
      geom_boxplot()
q9<-ggplot(data = dfCrime, aes(y = wloc, color = regcode)) + 
      geom_boxplot()
q10<-ggplot(data = dfCrime, aes(y = taxpc, color = regcode)) + 
      geom_boxplot()
grid.arrange(q1, q2, ncol=2)
grid.arrange(q3, q4, q5, q6, ncol=2)
grid.arrange(q7, q8, q9, q10, ncol=2)
```

We observe a few data points of interest in the comparison above, notably, wser appears to have an extreme data point.

Other variables show outliers as well, but not as extreme. We will determine if any of these points have leverage or influence during model specification.

For now, lets dig deeper into wser as we see it is an extreme outlier from our visual inspection.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime %>%
filter(wser > 2000) %>%
select(county, wser)
```

This average service wage is much too high based on what we know about the 1980s and every other wage recorded in comparison. A review of the detailed population statistics describing mean wage per industry (table 231) confirms this. https://www2.census.gov/prod2/decennial/documents/1980/1980censusofpopu801352uns_bw.pdf

Outliers affect our ability to estimate statistics, resulting in overestimated or underestimated values. Outliers can be due to a number of different factors such as response errors and data entry errors. Outliers will introduce bias into our estimates and are addressed during the analysis phase. The mechanism for treatment include three approaches 

1 Trimming - *remove* outliers from the dataset based on a maxima or minima from the mean.

2 Winsorization - *replace* extreme values so they fall at the edge of the main distribution

3 Imputation - *recode* outliers by calculating the mean of the sample, or by applying a regression model approach to predict the missing value

Trimming will remove the entire record. This is not an preferred treatment as we will lose valuable information.. 

Winsorization is a symmetric process that will replace *all* of the smallest and largest data values in the sample. This is not a preferred treatment as we will again lose valuable information, especially when we only seek to replace values that are a result of a coding error.

Imputation recodes the data point using a predictive model derived from the remainder of the sample data points. Multiple imputations are performed to account for uncertainty, and each imputed data set is analyzed for its distribution. Then, the mean, mode or median can be chosen from this distribution as the replacement.

We favor the imputation method as we do not wish to apply Winsorization to our data set wholistically, nor do we wish to remove the entire observation and lose its contribution to other variables. Fortunately, a number of packages are available in R that predict this value through regression against the existing sample data. A commonly used imputation library can be found in the Hmisc package which we will use for our outlier treatment.

A full discussion of treatment methods can be found here: http://www.asasrms.org/Proceedings/y2004/files/Jsm2004-000559.pdf

Finally, we should make note that none of these methods are ideal. We would be better suited to discover the nature of the mistake and recode it from real data. Since we do not have access to the underlying data from which this sample set was derived we are not in the position to do that. Thus we continue our analysis.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$wser[which(dfCrime$county==185)]<-NA # set the value to NA so it will be imputed
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE, results='hide'}
impute_arg <- aregImpute(~ crmrte +  urban + central + west + other +
                         prbarr + prbconv + prbpris + avgsen + polpc + 
                         density + taxpc + pctmin80 + wcon + wtuc +
                         wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                         mix + pctymle, data = dfCrime, match="weighted",
                         nk=3, B=10, n.impute = 100)
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("R-squares for Predicting Non-Missing Values for Each Variable")
impute_arg$rsq
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("Distribution of Values for Each Imputation")
table(impute_arg$imputed$wser)
```

Note from the distribution above we see a mode appear from the multiple imputation trials. One may argue that taking the mode is sufficient for replacement. Taking a mode would be required if this were a categorical value we were replacing. However, in this circumstance and to err on the side of caution, we will reassign this value by taking the mean from the trials.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$wser[which(dfCrime$county==185)]<-mean(impute_arg$imputed$wser)
print("Newly Reassigned wser Value for County 185:")
dfCrime$wser[which(dfCrime$county==185)]
```
Next, we will examine the criminal justice variables.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#Plot of the criminal justice and law enforcment related variables vs crmrte
q1<-ggplot(data = dfCrime, aes(y = prbarr, color = regcode)) + 
      geom_boxplot()
q2<-ggplot(data = dfCrime, aes(y = prbconv, color = regcode)) + 
      geom_boxplot()
q3<-ggplot(data = dfCrime, aes(y = prbpris, color = regcode)) + 
      geom_boxplot()
q4<-ggplot(data = dfCrime, aes(y = avgsen, color = regcode)) + 
      geom_boxplot()
q5<-ggplot(data = dfCrime, aes(y = polpc, color = regcode)) + 
      geom_boxplot()
q6<-ggplot(data = dfCrime, aes(y = mix, color = regcode)) + 
      geom_boxplot()

grid.arrange(q1, q2, ncol=2)
grid.arrange(q3, q4, q5, q6, ncol=2)
```

The criminal justice and law enforcement variables show evidence of outliers, notably, pbarr and polpc appear to have extreme data points.

Upon further inspection, the extreme outlier value for polpc is .009. Based on records describing the US population on police officers per capita, the highest police per capita on record for United States counties is .007 in Atlantic City, NJ. https://www.governing.com/gov-data/safety-justice/police-officers-per-capita-rates-employment-for-city-departments.html This datapoint is an error and we will impute it's replacement.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$polpc[which(dfCrime$county==115)]<-NA # set the value to NA so it will be imputed
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE, results='hide'}
impute_arg <- aregImpute(~ crmrte +  urban + central + west + other +
                         prbarr + prbconv + prbpris + avgsen + polpc + 
                         density + taxpc + pctmin80 + wcon + wtuc +
                         wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                         mix + pctymle, data = dfCrime, match="weighted",
                         nk=3, B=10, n.impute = 100)
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("R-squares for Predicting Non-Missing Values for Each Variable")
impute_arg$rsq
paste("Predicted values")
mean(impute_arg$imputed$polpc)
```

We will reassign this value using the mean from the trials.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$polpc[which(dfCrime$county==115)]<-mean(impute_arg$imputed$polpc)
```

Our analysis into the demographic data is next.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#plot of demographic information for counties Outside and Inside the metro areas
# population density, percent minority, percent young male

ggplot(data = dfCrime, aes(y = density, color = regcode)) + 
      geom_boxplot() + facet_wrap(~ metro)
ggplot(data = dfCrime, aes(y = pctmin80, color = regcode)) + 
      geom_boxplot() + facet_wrap(~ metro)
ggplot(data = dfCrime, aes(y = pctymle, color = regcode)) + 
      geom_boxplot() + facet_wrap(~ metro)
```

Notably more outliers are observed in demographic information. Here, pctymle in one county outside of a metro area is nearly 25%. That seems quite high in normal statistical measures of the population. However, this can be explained as being county with a large college town population. Specifically, Appalachian State University in Watauga County (https://en.wikipedia.org/wiki/Watauga_County,_North_Carolina), where the presense of the university notably affects the overall age distribution and median age. This is further confirmed in county estimate records here: https://www.osbm.nc.gov/demog/county-estimates.

Finally, we can see our CW encoded county and where it appears in population density. It is clearly not an outside metro county. In addition to being improperly coded for both western and central regions it appears to be miscoded for its metro characteristics as well.

We will address the metro variable, and examine whether the region should be 'west', 'central' or 'other' instead of both central and west


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime %>%
#filter(west ==1 & central ==1) %>%
filter(density > 2.5) %>%
select(county, west, central, other, urban, region, regcode, metro)
```

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$west[which(dfCrime$county==71)]<-NA
dfCrime$central[which(dfCrime$county==71)]<-NA
dfCrime$other[which(dfCrime$county==71)]<-NA
dfCrime$urban[which(dfCrime$county==71)]<-NA

```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE, results='hide'}
impute_arg <- aregImpute(~ crmrte +  urban + central + west +
                         prbarr + prbconv + prbpris + avgsen + polpc + 
                         density + taxpc + pctmin80 + wcon + wtuc +
                         wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                         mix + pctymle, data = dfCrime, match="weighted",
                         nk=3, B=10, n.impute = 100)
```

```{r, include=FALSE, comment=NA, warning=FALSE, echo=FALSE}
#We need a mode function, so lets define one. Source - public domain
Mode = function(x){ 
    ta = table(x)
    tam = max(ta)
    if (all(ta == tam))
         mod = NA
    else
         if(is.numeric(x))
    mod = as.numeric(names(ta)[ta == tam])
    else
         mod = names(ta)[ta == tam]
    return(mod)
}
```

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("R-squares for Predicting Non-Missing Values for Each Variable")
impute_arg$rsq
paste("Predicted values")
Mode(impute_arg$imputed$urban)
Mode(impute_arg$imputed$central)
Mode(impute_arg$imputed$west)
```

The results confirm the county is urban. It is also highly probable that county 71 is not west and most likely associated with central. After correcting our data for urban and west, let's compare 'central' with 'other' to be certain we have the right region.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$urban[which(dfCrime$county==71)]<-Mode(impute_arg$imputed$urban)
dfCrime$nonurban[which(dfCrime$county==71)]<-1-Mode(impute_arg$imputed$urban)
dfCrime$west[which(dfCrime$county==71)]<-Mode(impute_arg$imputed$west)
dfCrime$metro[which(dfCrime$county==71)]<-'Inside Metro'
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE, results='hide'}
impute_arg <- aregImpute(~ crmrte + central + other +
                         prbarr + prbconv + prbpris + avgsen + polpc + 
                         density + taxpc + pctmin80 + wcon + wtuc +
                         wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                         mix + pctymle, data = dfCrime, match="weighted",
                         nk=3, B=10, n.impute = 100)
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("R-squares for Predicting Non-Missing Values for Each Variable")
impute_arg$rsq
paste("Predicted values")
Mode(impute_arg$imputed$other)
```

We show with high degree of certainty that the county is not 'other'. The case for central is high. Since the county is not western and not other it is central by process of elimination, and the Hmisc algorithm bolsters that suggestion. We'll assign our new values.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$other[which(dfCrime$county==71)]<-Mode(impute_arg$imputed$other)
dfCrime$central[which(dfCrime$county==71)]<-1-Mode(impute_arg$imputed$other)
dfCrime$region[which(dfCrime$county==71)]<- 2 #Central
dfCrime$regcode[which(dfCrime$county==71)]<- 'Central'
```

We also wish to examine the density variable in more detail by looking at its distribution.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
options(repr.plot.width=8, repr.plot.height=4)
ggplot(data = dfCrime, aes(x = density)) + 
      geom_histogram(bins=90)
```

We note that one of the counties has an extremely low density. Near zero.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime %>%
filter(density < 0.01)
```

In review of the North Carolina county density data from 1985, the smallest population density in any county in North Carolina is 0.0952. http://ncosbm.s3.amazonaws.com/s3fs-public/demog/dens7095.xls

This makes the density of 0.0000203422 (ie. average of ~2.0 people per 10,000 square miles) for county 173 statistically impossible. It is miscoded.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$density[which(dfCrime$county==173)]<- NA
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE, results='hide'}
#dfSubset <-  we will use the non-urban western counties
impute_arg <- aregImpute(~ crmrte +
                         prbarr + prbconv + prbpris + avgsen + polpc + 
                         density + taxpc + pctmin80 + wcon + wtuc +
                         wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                         mix + pctymle, data = dfCrime %>% filter(urban==0 & west ==1),
                         match="weighted",  nk=3, B=10, n.impute = 30)
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("R-squares for Predicting Non-Missing Values for Each Variable")
impute_arg$rsq
paste("Predicted values")
mean(impute_arg$imputed$density)
```

We will reassign this value using the mean from the trials.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$density[which(dfCrime$county==173)]<-mean(impute_arg$imputed$density)
```

With our variables transformed, we now turn to discussion on collinearity and multicollinearity in our data set. To facilitate the discussion we'll draw reference to a network plot.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
options(repr.plot.width=6, repr.plot.height=6)
myData<-dfCrime
myData<-myData[, c("crmrte", "west", "central", "other", "urban", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "taxpc",
           "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
           "mix", "pctymle", "density")]
plot<-myData %>% correlate() %>% network_plot(min_cor=.2)
grid.arrange(arrangeGrob(plot, bottom = 'Correlations Among Variables'), 
             top = "Network plot for Correlation Study", ncol=1)
```
First, we note the general proximity of variables with one another. Variables that are clustered together represent the overall magnitude of their correlations. In fact, the cluster of the wage variables are an indication of very tight correlation. Only state wages fall outside this group. The telecom and utlity wage variable, while still near the cluster, show a little relationship.We also see the wage variables are positively correlated with our crime outcome variable. Density also positively correlates with wage and the crime rate variable. Urban correlates with wage, but suprisingly the correlation between crime and urban is not as high.

Next, we notice the Law enforcement and Judicial variables are clustered and have a negative correlation with our outcome variable on crime. We also see they tend to be negatively correlated among one another. For example, probability of conviction is slightly negatively corrrelated with the probability of arrest, and both are negatively correlated with our outcome variable. We also see that police per capita and tax per capita are positively correlated with another. This makes sense as the more revenues collected the higher the ability to pay for law enforcement and protection. Both are also positively correlated with our outcome variable on crime. We also notice that percent young male has a positive correlation with crime rate. A possible explanation for this is that more crimes are committed by younger men as a whole. We also note that the state wage variable is in nearby proximity.

The mix variable is an odd one. It is possitively correlated with probability of arrests, negatively correlated with probability of convictions, and negatively correlated with service and manufacturing wages. It also has a slight positive correlation with the state wage variable and seems to be clustered with it. 

Last, we turn to our region variables and notice the high negative correlation of the minority variable with the western region dummy variable. We also notice a high positive corrlation of minorities with the 'other' (coastal) dummy variable. The pctmin80 variable also correlates positively with crime rate, although the two are not clustered. We especially note that west is negatively correlated with crime rate. There appears to be a lessor propensity for crime in this region, or perhaps a lack of suffient means to detect it. For a futher examination of correlation plots for each of the regions please see the network diagrams in the appendix.

## Additional Variables to Operationalize

As a final point of discussion we will identify variables we wish to operationalize for use in our models. We will include a variable that expresses the economic condition of the county and a variable that expresses criminal justice effectiveness.

The first variable on the economic condition will include the sum of all average weekly wages from the 1980 census information. Since we do not know how many were employed at that wage we use this summary the best available proxy. Additionally, we will scale this wage variable by 9 to highlight indication of industry diversity within a county.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$scaledWages<-(dfCrime$wcon + dfCrime$wtuc + dfCrime$wtrd + dfCrime$wfir +
    dfCrime$wser + dfCrime$wmfg + dfCrime$wfed + dfCrime$wsta + dfCrime$wloc) / 9
```

As a second variable, we are interested in understanding the effectiveness of the criminal justice system as a crime deterrent. Our proxy will be the number of convictions per incident.

This is operationalized by taking the probability of arrests, pbrarr (which is defined as arrests per incident) and multiplying by the probability of convictions, pbrconv (which is defined as convictions per arrest). The new variable is defined below.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$crimJustEff<-dfCrime$prbarr * dfCrime$prbconv
```

We will also create a logarithmic transformation of this variable based on our histogram analysis from before.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logcrimJustEff<-log(dfCrime$crimJustEff)
```

## Summary and Results

Our outcome variable is the *crime rate* (“crmrte”), which is defined as the crimes committed per person in a specific county during 1987. The crime rate of the 90 counties in our sample dataset range between 0.0055 - 0.0990, with a mean of 0.0335.

From the boxplot below, most of the counties have a crime rate between 0.0055 and 0.0700, with 5 outliers having a crime rate > 0.0700.

```{r}
options(repr.plot.width=8, repr.plot.height=4)
p<-ggplot(data = dfCrime, aes(y = crmrte, color = regcode)) + 
     geom_boxplot(show.legend=FALSE) + facet_wrap(~ regcode)
p2<-ggplot(data = dfCrime, aes(y = crmrte)) +
     geom_boxplot()
grid.arrange(p, p2, ncol=2)
```

While mix (the type of crime committed) is also potentially an outcome variable, our research focuses on providing policy recommendations to reduce crime in general and not a specific type of crime. Mix is also not a linear outcome and hence difficult to measure. 

We propose 3 multiple linear regression models

* First Model: Has only the explanatory variables of key interest and no other covariates.

* Second Model: Includes the explanatory variables and covariates that increase the accuracy of our results without substantial bias.

* Third Model: An expansion of the second model with most covariates, designed to demonstrate the robustness of our results to model specification.

# Model Analysis

## Model 1 
### Introduction

Our base hypothesis is that county level crime can be fundamentally explained by three factors in county: regional variations, the effectiveness of the criminal justice system, and economic conditions.

* **Criminal Justice Effectiveness**  
Criminal Justice Effectiveness is an abstract concept that is operationalized by comparing the number of crimes to convictions. To track crimes, they must be reported to police, who can then make arrests. Then, the legal system provides judgement in the form of convictions and sentencing.  Besides removing some criminals from society, criminal justice can serve as deterrent, as the probability of getting caught, convicted, sentenced could discourage some would be criminals from committing crimes. 

>We operationalize criminal justice effectiveness as (probability of Convictions * Crimes committed). We define this as: prbconv * prbarr = conv/arrest * arrest/crime = convictions/crime. Without more granular data, this provides a single parsimonious metric that helps understand how well the law enforcement and criminal justice system works.

* **Region**  
Region is easily identified as possible factor for predicting crime as seen from our initial EDA in the Introduction.  It would be expected that cultural, econocomic, government, geographic, and demographic profiles among other important variables would vary across the regions. Without the ability to measure and operationalize these variables, region offers a solid standin.  

* **Economic Conditions**  
Finally, we theorize that the third major cause of crime are economic conditions.  We would expect that as someone's economic success and opportunity increases their propensity to commit crime is lowered.  And similarly when there are worse economic conditions, crime would increase due to lack of means, lack of occupation or boredom.  This also means that individuals have less to look forward to and are willing to risk their freedom or endanger themselves. 

>We operationalize economic conditions by looking at wages. For this base, parsimonious model, we define this as the unweighted average weekly pay from each sector provided in the data set.  We think this is best proxy from our data because it answers all of the above (higher wages leads to better means and better opportunities). From our EDA we also confirm that in general these sums are not skewed by having one really high paying sector in each county as we see a strong relationship between average quartile across all job types and unweighted sector average wage.  


### Model 1 EDA

**Data Transformations: Criminal Justice Effectiveness**  
First we look at the components of the Criminal Justice Effectiveness: Probability of arrest and Probability of conviction.  

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
p1 <- ggplot(dfCrime, aes(x = prbarr, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Probability of Arrest", x="Arrest per Crime", y="Count")
p2 <- ggplot(dfCrime, aes(x = prbconv, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Probability of Conviction", x="Conviction per Arrest", y="Count")
grid.arrange(p2, p1, ncol=2)
```

The distribution of both probability of conviction and probability of arrest are skewed right.  It could be argued that both of these variables should be bound between 0 and 1.  However, these "probabilities" are proxied by ratios.  It is in fact possible (and perhaps common) that defendents are charged with multiple crimes and convicted, but were only arrested once. For this reason we will not consider outliers in this variable.

For "probability" of arrest, it could be possible there are multiple arrests for a single crime. However, the single data point that is greater than one, is >5 standard deviations away from the distribution. Since this value falls so far out of distribution, it will have high leverage on our model and will be preemptively imputed as the data supplied is likely in error and is not representative of the bulk of North Carolina counties. 


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
# how many standard deviations away the outlier lies
dfCrime[dfCrime$crimJustEff > 1,]$county
(dfCrime[dfCrime$crimJustEff > 1,]$prbarr - mean(dfCrime$prbarr))/sd(dfCrime$prbarr)
```

We will use the imputation method to replace the large prbarr value and remove the outlier effect, while also retaining the rest of the variables in the county.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$prbarr[which(dfCrime$county==115)]<-NA # set the value to NA so it will be imputed
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE, results='hide'}
impute_arg <- aregImpute(~ crmrte +  urban + central + west + other +
                         prbarr + prbconv + prbpris + avgsen + polpc + 
                         density + taxpc + pctmin80 + wcon + wtuc +
                         wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                         mix + pctymle, data = dfCrime, match="weighted",
                         nk=3, B=10, n.impute = 100)
```


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
paste("R-squares for Predicting Non-Missing Values for Each Variable")
impute_arg$rsq
paste("Predicted values")
mean(impute_arg$imputed$prbarr)
```

We will reassign this value using the mean from the trials.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$prbarr[which(dfCrime$county==115)]<-mean(impute_arg$imputed$prbarr)
```


With the outlier imputed, the Criminal Justice Effectiveness can be constructed as described above, and a histogram can show how log transformation improves normality.  Also, by log transforming we can understand the resulting coefficient as a percent change in the ratio of Criminal Justice Effectiveness (convictions/crime). 

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$crimJustEff<-dfCrime$prbarr * dfCrime$prbconv
dfCrime$logcrimJustEff<-log(dfCrime$crimJustEff)
dfCrime$logcrmrte <- log(dfCrime$crmrte)
options(repr.plot.width=4, repr.plot.height=4)
p1 <- ggplot(dfCrime, aes(x = crimJustEff, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Criminal Justice Effectiveness", x="Convictions per Crime", y="Count")
p2 <- ggplot(dfCrime, aes(x = logcrimJustEff, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="log(Criminal Justice Effectiveness)", x="log(Convictions per Crime)", y="Count")
grid.arrange(p1, p2, ncol=2)
```

**Data Transformations: Unweighted Average of Sector Wages**

Now we turn our attention to the economic variable, unweighted average wage of all provided sectors. The wages trend together well, so there is limited information lost when combining them into one variable.  This can be seen by plotting quartiles of each wage against the unweighted average and observing a relatively linear response.  

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
# # Quantiles for all jobs
dfWage<-mutate(dfCrime,qCon=ntile(dfCrime$wcon,4))
dfWage<-mutate(dfWage,qTuc=ntile(dfCrime$wtuc,4))
dfWage<-mutate(dfWage,qTrd=ntile(dfCrime$wtrd,4))
dfWage<-mutate(dfWage,qFir=ntile(dfCrime$wfir,4))
dfWage<-mutate(dfWage,qSer=ntile(dfCrime$wser,4))
dfWage<-mutate(dfWage,qMfg=ntile(dfCrime$wmfg,4))
dfWage<-mutate(dfWage,qFed=ntile(dfCrime$wfed,4))
dfWage<-mutate(dfWage,qSta=ntile(dfCrime$wsta,4))
dfWage<-mutate(dfWage,qLoc=ntile(dfCrime$wloc,4))
## Average quantile
dfWage$qAvg= (dfWage$qCon+dfWage$qTuc+dfWage$qTrd+dfWage$qFir+dfWage$qSer+dfWage$qMfg+
                dfWage$qFed+dfWage$qSta+dfWage$qLoc)/9

plot(dfCrime$scaledWages,dfWage$qAvg)
```

We will again modify this variable with a log transformation for better interpretation. This allows us to interpret percent changes in wage which is more consistent across a range of wages. The result is a relatively normal distribution as seen in the below histogram.  There exists a small "second mode" for the Central region, but this is not very concerning. 


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logScaledWages <- log(dfCrime$scaledWages)
p1 <- ggplot(dfCrime, aes(x = logScaledWages, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="log(Scaled Wages)", x="log(Scaled Wages)", y="Count")
p1
```
Interestingly, when viewing the wage data plotted against crime rate (below).  We see that there is a positive correlation between wages and crime.  We will see if this holds when taking into account the Criminal Justice Effectiveness in model 1 and discuss some possible causes.  
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}

q9<-ggplot(data = dfCrime, aes(x = logScaledWages, y = logcrmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
options(repr.plot.width=8, repr.plot.height=16)
q9
```

**Data Transformations: Regions**  
Finally, regions will be used as explained in the EDA section. No transformations are necessary.

### Model 1 Linear Model

$$ log(Crime Rate) = B_0 + B_1log(Unweighted Average Wage) + B_2log(Criminal Justice Effectiveness) + B_3West + B_4Central$$

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}

#dfCrime$unweighted_avg_wage <- dfCrime$scaledWages/9
mod1 <- lm(logcrmrte ~ logScaledWages + logcrimJustEff + regcode, data=dfCrime)
mod1 # Coefficients
summary(mod1)$adj.r.square # Adjusted R^2 value.

```
**Cook's Distance (Leverage/Influence) Analysis**  
None of the points approach a cook's distance of concern.  Some of the higher leveraged points do trend towards larger negative residuals.  This suggests that our model might not be capturing a phenomenon at more extreme values of the regressors. For an initial model there is nothing of major concern.
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
plot(mod1, which=5) # Variance Inflation Factor
```

**Model 1 CLM Assumptions:**  
* **MLR1** Linear in parameters: The model has had its data transformed as described above to allow a linear fit of the model. 

* **MLR2** Random Sampling: The data is collected from a data set with rolled up data for each county.  We cannot comment on the randomness of the samples, though nearly all counties are represented in North Carolina.  

* **MLR3** No perfect multicollinearity: None of the variables chosen for the model are constant or perfectly collinear as the economy and criminal justice effectiveness are independent.  Our low VIF value shows very little colinearity, as would be expected from the diverse and limited variables included in the model.
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
vif(mod1) # Variance Inflation Factor
```
* **MLR4'** The expectation of u is 0.  This is difficult to prove in a data set like this one.  It is possible that there are serious bias issues in the way crimes are reported and wages are recorded.  However, if one agrees that the data has integrity and that the basic model presented for predicting crime rate is acceptable, then exogeneity can be accepted.  

* **MLR4** The zero conditional mean assumption is well supported when viewing the Residuals vs fitted plot.  The spline fit is nearly flat and centered very close to zero.  
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
plot(mod1, which=1)
```

* **MLR5** There does appear to be heteroskedacity in the 'lips' appearance of the Residuals vs fitted plot.  A Breusch-Pagan test shows that there is not heteroskedacity. However, to be conservative, we will proceed with heteroskedastic robust errors.
```{r}
bptest(mod1)
coeftest(mod1, vcov=vcovHC) #coefficients with heteroskedastic consistent standard errors
```
* **MLR6** The final assumption of linear regression is that the errors are normally distributed.  This appears to hold for the bulk of the residuals however there is skewness on the tails.  This non-normality is also reflected in the significant return on the shapiro test.  Our high sample size allows us to leverage the Central Limit Theorem and assume normally distribued errors. 
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
shapiro.test(mod1$residuals) # test for normality
plot(mod1, which=2) # QQ plot for residuals
```

### Model 1 Interpretation
The model 1 gives estimates and standard errors that are heteroskedastic consistent.  All the coefficients are found to be statistically significant. The coefficient of the log of unweighted average wage is calculated to be ~2.  This means that an increase of 1% in wages is correlated with an increase of 2% in crime rate.  Generally an individuals increased wages are not associated with increased crime.  This suggests that wages are correlated with a stronger omitted variable that affects crime.  One aspect that may be missed is the economic inequality in the county.  Averages are easily affected by extreme values.  It is possible that there are very high earners that can influence a sector's average wage, but don't represent the bulk of workers.  Further, there is no understanding of the weighting for each sector.  For example, one sector, like telecom, may have high wages in a county but not have very many workers; further demonstrating inequality.  This detail is missed when each sector is rolled up into a single average and then averaged with all other sector averages.  However, the significant result of wage on crime suggests that we are capturing some cause of crime that is correlated to wages.  We will continue to monitor how wage changes as a predictor when introducing more regressors in subsequent models.

Criminal justice effectiveness (convictions/crime) is given a coefficient of -0.5 which suggests that an increase of 1% increase in convictions per crime will decrease crime by nearly .5%.  This suggests that we have found a relatively strong correlation and constructed a good operationalization of a county's criminal justice effect on crime rate. This variable will be monitored as we add more regressors. 

Region dummy variables of West and Central are both significant.  This suggests that regionally, there are differences that are not captured by the Wages and Criminal Justice Effectiveness variable that affect crime.  While the West and Central regions both have lower crime than the Coastal/Other region, the West is much more pronounced with a value of -0.6.  This suggests that when correcting for differences in Criminal Justice Effectiveness and Wages, the west will still have crime rate lower by about 60%.

Overall, the model shows a moderately good fit, with an adjusted R square of 0.63.  This can be interpreted as: the model explains 63% of the variation in crime.  In the next model we will try to improve our operationalization of economics and criminal justice by investigating police per capita and tax revenue per capita. 

## Model 2

### Introduction
In this model, we introduce the additional covariates of tax per capita (taxpc) and police per capita (polpc) to increase the accuracy of our regression. We are including these additional variables to our second model, as they add accuracy to the explanatory variables used in our first model: 

--- why we are removing regcode as a direct variable

1. The **Police Per Capita** in a county can be influential on the Criminal Justice Effectiveness. With more police in a given area, one would think that crime rates would decrease, however our correlation plot below tells a different story. Including this variable in our analysis will give us more insight into the variables used in model 1. 

2. The **Tax Per Capita** can have a direct impact on the Police Per Capita. A higher tax per capita, means that the county has more tax dollars to spend on public services like the police force (ie. increasing the number of police in the county). 

-- correlation discussion. Also could mean economic activity (look at mod1) or overbearing sales tax?

$$logcrmrte = \beta_0 + \beta_1log(Unweighted Average Wage) + \beta_2log(Criminal Justice Effectiveness) + \beta_3log(Police Per Capita)*West\\ +  \beta_4log(Police Per Capita)*Central  + \beta_5logtaxpc + u$$

### Model 2 EDA and Data Transformations

Before we create our model, we will analyze each of the additional variables to see if transformations are needed. 

To start, we will look at the polpc variable:
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
p1 <- ggplot(dfCrime, aes(x = polpc, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Police per Capita", x="Police per Capita", y="Count")
p2 <- ggplot(dfCrime, aes(x = log(polpc), color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Log Transformed Police per Capita", x="Log(Police per Capita)", y="Count")
grid.arrange(p1, p2, ncol=2)
```

As we can see from the histograms above, taking the natural log of polpc brings its distribution closer to normal. As a result, we will use the log(polpc) in our analysis.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
# creating the logpolpc variable
dfCrime$logpolpc <- log(dfCrime$polpc)
```

Next, we will take a look at our taxpc variable to see if a trasformation is warranted:

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
p1 <- ggplot(dfCrime, aes(x = taxpc, color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Tax revenue per capita", x="Tax revenue per Capita", y="Count")
p2 <- ggplot(dfCrime, aes(x = log(taxpc), color=regcode, fill = regcode)) + 
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(title="Log Transformed Tax revenue per Capita", x="Log(Tax revenue per Capita)", y="Count")
grid.arrange(p1, p2, ncol=2)
```
The histogram of taxpc, depicts each region as having an approximately normal distribution with each cenetred on a different mean. When we take the natural log of taxpc, we can see in the histogram, above, that each region has a more normal distribution. In addition, taking the natural log of taxpc reduces the extremity of the outlier seen in histogram of taxpc. As a result, we will use the log(taxpc) in our analysis. 
      
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logtaxpc <- log(dfCrime$taxpc)
```

Now that we have transformed the two variables we are adding to model two, we will take a look at how logtaxpc and logpolpc relate to logcrmrate and if these trends vary between each regcode.

-- SHOULD WE MENTION WHY WE ARE LOOKING AT THIS BY REGCODE --

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}

logpolpc_plot<-ggplot(data = dfCrime, aes(x = logpolpc, y = logcrmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
logtaxpc_plot<-ggplot(data = dfCrime, aes(x = logtaxpc, y = logcrmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")

options(repr.plot.width=8, repr.plot.height=16)
grid.arrange(logpolpc_plot,logtaxpc_plot, ncol=2)
```

Right away, we can see clear difference between each regions' logpolpc regression on logcrmerte."Other" and "Central" regions have positively sloped regression lines, while "West" has a negatively sloped regression line. This suggests that we should investigate the interactions between logpolpc and region (regcode) in our model. 

logtaxpc to logcrmrte, on the other hand, does not demonstrate this regression slope variation between each regions. As result, we will not look into the interactions between logtaxpc and region in our model. 

-- DELETE -- 
NOTES: -regionally we see a difference in the polpc regression on crime rate
      - this suggests that we should investigate the interaction between polpc and region. --> law enforcement style could be different in the regions which.  
      - we do not need to look at taxpc and region as the slopes do not vary greatly across regions.
-----      

### Model 2 Linear Model

$$logcrmrte = \beta_0 + \beta_1logScaledWages + \beta_2logcrimJustEff + \beta_3logpolpc * regcode + \beta_4logtaxpc + u$$

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
model2 <- lm(logcrmrte ~ logScaledWages + logcrimJustEff + logpolpc*regcode + logtaxpc, data = dfCrime)
model2
summary(model2)$adj.r.square
```

From the Residuals vs Leverage graph, below, we can see that our model does not contain any outliers with have significant influence (ie. there are no points with a Cook's distance of 0.5 or greater).

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
plot(model2, which=5) 
```

**Model 2 CLM Assumptions:**

* **MLR1** Discussed above.
* **MLR2** Discussed above.

* **MLR3: Non-perfect Collinearity** We will use the VIF function to provide evidence that our variables in model2 are not perfectly multicollinear. As we can see from the VIF results, below, all of the variables' values are less than five, which allows us to conclude model2 is free from multicollinearity.
```{r}
vif(model2)
```

* **MLR4: Zero Conditional Mean** The residual vs. fitted chart, below, gives us evidence that we meet the zero conditional mean assumption as the majority of the residual means lie close to zero. The exceptions to this trend, lie on the left and right sides of the chart where there are fewer data points (evidence for heteroscedasticity - see MLR5, below). 
```{r}
plot(model2, which=1)
```

* **MLR5: Homoscedasticity** The above Residuals vs Fitted graph provides evidence of heteroscedasticity as right side of the chart have fewer datepoints. To provide further evidence of heteroscedasticity, we will use the White-Huber test with the vcovHC method to generate coefficients that are robust to heteroscedasticity

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
coeftest(model2, vcov=vcovHC)
```

As we can see from the coeftest results above, the interactions between logpolpc and regcode west and central are not statistically significant (at least by themselves).  

However, they may be jointly significant. We will run a f-test to see if this might be the case:

```{R}
linearHypothesis(model2, c("logpolpc:regcodeWest=0", "logpolpc:regcodeCentral"), vcov=vcovHC)
```

We can see frome the f-test, above, that the interactions between logpolpc and regcodes west and central are not statistically significant to the model. 

-- DELETE ---
NOTES: INTERACTIONS BETWETEEN POLPC AND REGCODE WEST AND CENTRAL ARE NOT BY THEMSELVES SIGNIFICANT.  LETS F TEST TO SEE IF THEY ARE JOINTLY SIGNIFICANT...and they are barely not significant.  The dummy variable for central is not significant which suggests that central and other are very similar while west is different.
---

* **MLR6: Normal Distribution of Errors** The Normal Q-Q plot, below, provides evidence that our residuals follow a normal distribution. While there are some data points on the left and right side of the graph that stray from the diagonal line, since our data set has over 30 datapoints, per the CLT, we can assume residuals have a normal distribution. 

```{r}
plot(model2, which=2)
# hist(model2$residuals)
#shapiro.test(model2$residuals)
#null hypothesis: residuals drawn from population with a normal distribution. 
#small p-value tells you if you can reject the null hypothesis. 
#this test depends on sample size, it does not take very much deviation from normality for
#us to get a statistically significant result
```


```{r}
summary(model2)
```

The Adjusted R-squared variable penalizes for additional variables, which means there is a chance that this value will decrease if the added variables do not contribute to the model. By comparing the Adjusted R-squared value between our first and second models, we see that logtaxpc and the interaction between logpolpc and regcode help describe logcrmrte. 

Our second model has an Adjusted R-squared value of 0.6989, which means 69.89% of the variation in the natural log of the crime rate is explained by the explanatory variables used in this model. This is a slight increase compared to our first model, that has an Adjusted R-squared value of 0.6345. 

In addition, the F-statistic is 26.83 with a statistically significant p-value of < 2.2e-16. As a result, we reject the null hypothesis that none of the independent variables help to describe logcrmrte.

Coefficient Analysis (assuming ceterus paribus):
- logcrimJustEff: -0.1607. This suggests that for a 1% increase in criminal justice efficiency, there is a 0.1607% decrease in crime rate. 
- logpolpc: 0.3701. This suggests that for a 1% increase in police per capita, there is a 0.3701% increase in crime rate. 
- scaledWages: 0.00006692. This suggests that for a 1% increase in total average weekly wage, there is a 0.0067% increase in crime rate.
- taxpc: -0.001632. This suggests that for a 1% increase in tax per capita, there is a 0.1632% decrease in crime rate.
- density: 0.06259. This suggests that for a 1% increase in density, there is a 6.259% increase in crime rate.

### Conclusion : Are the conclusions they draw based on this evaluation appropriate? Did the team interpret the results in terms of their research question?
Compared to model 1, the adjusted $R^2$ of model 2 is only marginally higher. This suggests that we should continue our analysis by focusing on the join significance of the variables added in model 2.  



## Model 3

### Discussion of Variables
From Model 2, we noted that the addition of the transformed variable logpolpc had statistical significance and helped improve the fit of the model, as measured by adjusted R-squared, to 70%. To increase our understanding at the linkages between police presence, economic conditions, criminal justice effectiveness and region, we propose to also analyse the areas of demographics which could have an effect on both of our key explanatory variables.  

**Minorities**
One key component of demographics is the race of the county inhabitants and how they are perceived and treated by others, especially for minorities in the population.  For example, systemic racism could have an important effect on: 
* Criminal Justice Effectiveness: If police, lawyers and judges are racially biased, this could lead to more arrests and more convictions regardless of the strength of the legal case and the evidence. As a result, we hypothesize the crime rate would increase.
* Economic Opportunity: Racism could prohibit members of the minority from having access to education, jobs and higher wages. Racism could also limit access to healthcare and social programmes which has a negative effect on economic opportunity. 

However, since we cannot directly measure racism, we have to operationalize this covariate by examining its effect in the real world. We propose to use the variable pctmin80, which represents the percentage of minorities in the population of the county. This is also a continous parameter and so given a higher the percentage of minorities, we should expect to see a greater effect. 

From the summary and boxplot below, we can see that the percentage of minorities ranges from 0.0154 - 0.6435, with a mean of 0.2621. We note that there are no major outliers. 
In addition, different regions and counties can have different demographics. We can see from the boxplots below that counties in the West have a significantly lower percentage of minorities than the other two regions. 

We will apply the natural log to the variable pctmin80 to 1) make it easier for us to interpret the coefficient in our linear model and 2) to better expose the linear relationship in the model. 
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
summary(dfCrime$pctmin80)
options(repr.plot.width=8, repr.plot.height=4)
p<-ggplot(data = dfCrime, aes(y = pctmin80, color = regcode)) + 
     geom_boxplot(show.legend=FALSE) + facet_wrap(~ regcode)
p2<-ggplot(data = dfCrime, aes(y = pctmin80)) +
     geom_boxplot()
grid.arrange(p, p2, ncol=2)

dfCrime$logpctmin80 <- log(dfCrime$pctmin80)
```

**Density**
Another component of demographics is the population density, which can have impacts that may be positive or negative on the crime rate:
    - **Criminal Justice Effectiveness**: With more people in a given area, there may be more opportunities for more people to commit crimes. However, a higher density may also result in higher deterents such as more eyewitnesses or faster law-enforcement response rates. 
    - **Economic Opportunity (ie. scaledWages)**: In high density areas, there is an increase in demand for support services such as food, retail, utilities, etc. As a result, there is a high demand for service jobs, which increases the economic opportunities within the area.  However, more people in a given area, there is a closer proximity to drugs, alcohol and gang violence - all of which are inhibitors to better economic outcomes. 

From the summary and boxplot below, we can see that the density ranges from 0.3006 - 8.8277, with a mean of 0.9792 persons per square mile. We note that while there are some outliers to the data, this does not appear at first glance to be an issue as some counties can contain major cities which would naturally lead to a higher population density. As a result, we will not make adjustments to any outliers unless we detect datapoints having high influence in our model.
```{r}
summary(dfCrime$density)
options(repr.plot.width=8, repr.plot.height=4)
p<-ggplot(data = dfCrime, aes(y = density, color = regcode)) + 
     geom_boxplot(show.legend=FALSE) + facet_wrap(~ regcode)
p2<-ggplot(data = dfCrime, aes(y = density)) +
     geom_boxplot()
grid.arrange(p, p2, ncol=2)
```


**Variables not considered**
We have also chosen not to include other variables from our dataset in our model:
* Urban: We believe the variable "density" better explains the same effects as "urban" while also being a continous. We believe a continous variable is more meaningful rather a binary indicator such as urban as there may be data points that failed to meet the cutoff for being defined as urban, but may still see the same effects as being urban and hence may distort our analysis.
* Age and Gender: While age and gender are important demographic variables, the only variable in our dataset is pctymle which provides the percentage of young males in the population. However, given that this variable encompasses both male and young, we may not be able to discern if age or gender has the larger effect (if any at all). 
* Judgement: We chose not to include the varibles concerning the probability of a prison sentence as well as the average sentence as we believe it is unlikely that potential criminals would have good access to this information. In addition, local county officials have limited influence over the decisions of the judiciary system, as they are separate branches of government.

Our equation for model 3 is as follows:
$$log(crmrate) = \beta_0 + \beta_1log(scaledWages) + \beta_2log(crimjusteff) + \beta_3regcode + \beta_4log(polpc) + \beta_5log(taxpc)+ \beta_6log(pctmin80) + \beta_7density +u$$

### Model 3 Linear Model
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
model3_initial<-lm(logcrmrte ~ logScaledWages + logcrimJustEff   +  regcode + logpolpc + logtaxpc + logpctmin80 + density, data = dfCrime)
summary(model3_initial)
```
We note from the the summary above and the F-tests below that west and central were not statistically significant to the model, but the inclusion of logpctmin80 and density are significant at the 99% confidence level. It appears that our new variables better explain the variation between counties rather than purely based on their geographic location and we thus remove the latter 2 variables from our model. 

```{r}
linearHypothesis(model3_initial,c("regcodeWest=0","regcodeCentral=0"), vcov=vcovHC)
vif(model3_initial)
```
Our revised equation for model 3 is as follows:
$$log(crmrate) = \beta_0 + \beta_1log(scaledWages) + \beta_2log(crimjusteff) + \beta_3log(polpc) + \beta_4log(taxpc)+ \beta_5log(pctmin80) + \beta_6density +u$$
```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
model3<-lm(logcrmrte ~ logcrimJustEff + logScaledWages +  logpolpc + 
              logpctmin80 + density, data = dfCrime)
summary(model3)
summary(model3)$adj.r.square
```

From the Residuals vs Leverage plot below, we also note that there are despite some data points have more leverage than others, no major outliers that have significant influence on our model as measured by no points having a Cook's distance > 0.5. 
```{r}
plot(model3,which=5)
```

**Model 3 CLM Assumptions:**

* **MLR1 and 2**: Discussed earlier.

* **MLR3** No perfect multicollinearity: We demonstrate that our independent variables are not perfectly multicollinear using the VIF function, and note that all of our variance inflation factors are less than 5.

```{r}
vif(model3)
```

* **MLR4'** Zero Conditional Mean: From the residual vs. fitted chart below, we see that the mean of the residuals mostly lie along 0, except towards the right side of our chart where there are fewer data points. We can reasonably conclude that we satisfy MLR4.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
plot(model3, which = 1)
```

* **MLR5'** Spherical errors: We note from the residuals vs fitted chart above that we have some evidence of heteroscedasticity, since there are less datapoints on both the left and right of the chart. As a result, we use the vcovHC method to estimate a robust variance-covariance matrix using White and Huber's method and generate coefficients that are robust to heteroscedasticity. Given that the coefficients in our model are fairly small, we know that the robust coeffcients generated will be appropriate for our analysis. 

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
coeftest(model3, vcov=vcovHC)
```

* **MLR6'** Normality of errors: From the qqplot below, we see that the residuals in our model follow a fairly normal distribution. In addition, since we have a large sample size of 90 datapoints, we can rely on a version of the central limit theorem to assume normally distributed errors.

```{r}
plot(model3,which=2)
```


By satisfying these assumptions, we can expect that our coefficients are approaching the true parameter values in probability. 

### Analysis
The model shows a good fit, with an adjusted R-squared of 0.72, meaning that the model explains 72% of the variation in crime. 

After accounting for coefficients that are robust to heteroscedasticity, we note only three them have individual statistical significance at the 95% level or better. These are criminal justice efficiency, minority percentages and density. However, running a F-test on the other two variables logpolpc and logScaledWages show that jointly they are still significant for our model. 

```{r}
linearHypothesis(model3,c("logpolpc=0","logScaledWages=0","logtaxpc=0"), vcov=vcovHC)
```
**Interpretation of coefficients (Assuming ceterus paribus):**

Positive coefficients:
* Police presence: If we increase police per capita by 1 percent, we expect the crime rate to increase by 0.28%.
* scaledWages: If we increase wages by 1 percent, we expect the crime rate to increase by 0.96%
* Density: If we increase density by 1 person per square mile, we expect the crime rate to increase by 7.48%
* Percentage of minorities: If the percentage of minorities increase by 1%, we expect the crime rate to increase by 0.25%

Negative coefficients:
* Criminal justice efficiency: If we increase the criminal justice efficiency by 1%, we expect the crime rate to decrease by 0.43%.

Of these different variables, we should pay particular attention to density given its large practical effect and statistical significance, which we address in our policy recommendations section below. 

## Comparison of Regression Models

***Can anyone figure out why logcrimJustEff is on 2 lines?**


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#*** Function to convert coeftest results object into data frame
ctdf=function(x){
  rt=list()                             # generate empty results list
  for(c in 1:dim(x)[2]) rt[[c]]=x[,c]   # writes column values of x to list
  rt=as.data.frame(rt)                  # converts list to data frame object
  names(rt)=names(x[1,])                # assign correct column names
  rt[,"sig"]=symnum(rt$`Pr(>|z|)`, corr = FALSE, na = FALSE,
                    cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                    symbols = c("***", "**", "*", ".", " "))
  return(rt)
}
# Get vectors of robust standard errors from the coeftest output
se.model1 <- ctdf(coeftest(mod1, vcov=vcovHC))[,"Std. Error"]
se.model2 <- ctdf(coeftest(model2, vcov=vcovHC))[,"Std. Error"]
#model23<-lm(logcrmrte ~ logcrimJustEff + logpolpc + logScaledWages + logpolpc*regcode + logtaxpc +density + logpctmin80, data = dfCrime)
#se.model23 <- ctdf(coeftest(model23, vcov=vcovHC))[,"Std. Error"]
se.model3 <- ctdf(coeftest(model3, vcov=vcovHC))[,"Std. Error"]
# Pass the standard errors into stargazer
#stargazer(mod1, model2, model23, model3, type = "text", omit.stat = "f",
#          se = list(se.model1, se.model2, se.model23, se.model3),
#          star.cutoffs = c(0.05, 0.01, 0.001))

stargazer(mod1, model2, model3, type = "text", omit.stat = "f",
          se = list(se.model1, se.model2, se.model3),
          star.cutoffs = c(0.05, 0.01, 0.001))


# waldtest(mod1, model2, vcov=vcovHC)
# waldtest(model2, model23, vcov=vcovHC)
# waldtest(model23, model3, vcov=vcovHC)
# 
 model4<-lm(logcrmrte ~ logcrimJustEff + logpolpc + logScaledWages + logpolpc*west  +density + logpctmin80, data = dfCrime)
 coeftest(model4, vcov=vcovHC)
 summary(model4)$adj.r.square
 linearHypothesis(model4,c("logpolpc:west=0", "west=0"), vcov=vcovHC)
```

Comparing the 3 models, we see that our adjusted R2 value has steadily increased from 0.456-0.732 as we introduce more covariates which indicates that we were able to explain more variation in our model not purely by increasing the number of indepedent variables. 

At the same time, our standard errors have decreased **insert more commentary on standard errors**.

We see that by expanding our definitions of criminal justice efficiency  and economic opportunity between model 1 and model 3 lowered the coefficients for logcrimJustEff and scaledWages. This is most likely because that we were able to better explain the effects with our newer variables.

Comment on practical significance after week 12

# Conclusion

## Policy Recommendations

Given that across all 3 models, we show that both criminal justice efficiency and tax revenues per capita have negative correlations to crime rate, we propose the policy recommendations below to address these issues. In addition, since minority percentages and density were found to be highly significant in the model 3, we believe our recommendations will be of particularly help to those  running for political office in counties with a high percentage of minorities or dense urban populations.

1. Since increasing both criminal justice and tax revenues are negatively correlated, we propose providing more funding for the local justice system. 

2. While increasing taxes on constituents may be difficult politically and may cost candidates the ballot, candidates can instead try to attract investment to bring more jobs with higher wages so you can increase revenues.

3.  Candidates can also propose to levy taxes on things that could lead to crimes or violence such as alcohol and weapons.

4. Given the significance and relatively large coefficient size of percentage minority, candidates should enroll local law enforcement into bias training.

## Ommitted Variables

| Expected correlation between omitted and included variables |
| ---- |

| Omitted Variable | Crime Rate ($B_k$) | Criminal Justice Effectiveness | Economic Conditions |
|------------------|--------------------|--------------------------------|---------------------|
| Education        | -                  |              unknown           | +                   |
| Social Services  | -                  |              unknown           | unknown             |
| Unemployment     | +                  |              unknown           | -                   |
| Inequality       | +                  |              unknown           | -                   |
| Gang Activity    | +                  | -                              | -                   |

The 4 major identified ommited variables are shown above. 

* Education is an important variable because of demographic insights it provides.  First, adults with higher education are less likely to participate in Crime and are more likely to have better economic opportunity. Second, a strong school system is also likely correlated with less youth crime.  Because of these expected correlations we are likely overestimating the economic conditions coefficient estimate. 
* Available Social Services could also lower crime.  Citizens with strong social services support have more options to get help when they lack means for purchasing basic life needs.  However this is more difficult to predict, as some social service projects, like homeless shelters, could lead to more criminal activity.
* Unemployment is used as an important indicator of economic health and opportunity.  This is would be highly correlated to economic conditions variables like sum of wages.  This indicator variable if added to the model would decrease the magnitude of the sum of wage means coefficient estimate.  
* Economic Inequality may also increase the crime rate as it may provide incentives for certain types of crime such as theft, kidnapping or extortion by people who have less economic means on those who have more economic means. 
* Gang or Organized Crime is special case of crime that contains unique causes.  It is expected that it would be negatively correlated with criminal justice effectiveness as large social pressures prevent witnesses from supporting prosecution.  Gang crime is also negatively correlated with economic conditions.  From these assumed correlations, we can say that criminal justice effectiveness and economic conditions are both underestimated compared to including gang activity operationalized variable in the model.

## Research Recommendations

We have shown in this report 3 different models that seek to explain and model changes in the crime rate in North Carolina in 1980. We start with the fundamental premise that crime is caused by both criminal justice efficiency and economic conditions, and further develop our definition of these two key explanatory variables which each new model. 

In Model 3, we were able to explain up to 73% of the variation in our data, and found statistical significance at the 95% level or better for each of our covariates. Of these, we believe that increasing the efficiency of the criminal justice system and tax revenues were the most important, particularly for counties with high density and minority populations. However, our findings should be noted with caution as we were unable to study the effect of several ommitted variables including education, availability of social services, unemployment rates and the presence of organized crime. Had we been able to collect data on these variables and apply them in our model, we believe we could increase accuracy without bias.



# Appendix


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
options(repr.plot.width=8, repr.plot.height=4)
#myData<-myData[, c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc",
#           "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
#           "mix", "pctymle")]
myData<-dfCrime %>% filter(other==1)
myData<-myData[, c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc",
          "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
          "mix", "pctymle")]
r0 <- myData %>% correlate() %>% network_plot(min_cor=.25)
myData<-dfCrime %>% filter(central==1)
myData<-myData[, c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc",
          "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
          "mix", "pctymle")]
r1 <- myData %>% correlate() %>% network_plot(min_cor=.25)
myData<-dfCrime %>% filter(west==1)
myData<-myData[, c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc",
          "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
          "mix", "pctymle")]
r2 <- myData %>% correlate() %>% network_plot(min_cor=.25)
grid.arrange(arrangeGrob(r1, bottom = 'Central Region Correlation Plot'), ncol=1)
grid.arrange(arrangeGrob(r2, bottom = 'Western Region Correlation Plot'), ncol=1)
grid.arrange(arrangeGrob(r0, bottom = 'Other Region Correlation Plot'), ncol=1)



myData<-dfCrime %>% filter(urban==0)
myData<-myData[, c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc",
          "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
          "mix", "pctymle")]
r0 <- myData %>% correlate() %>% network_plot(min_cor=.25)
myData<-dfCrime %>% filter(urban==1)
myData<-myData[, c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc",
          "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc",
          "mix", "pctymle")]
r1 <- myData %>% correlate() %>% network_plot(min_cor=.25)
grid.arrange(arrangeGrob(r0, bottom = 'Non-Urban Correlation Plot'), ncol=1)
grid.arrange(arrangeGrob(r1, bottom = 'Urban Correlation Plot'), ncol=1)

```


##  Transformations

Transform examinations through ggplots. Feel free to cherry pick what you need. All of these plots will be removed in the final report.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#dfEconVars <- as.data.frame(cbind(dfCrime$wcon, dfCrime$wtuc, dfCrime$wtrd, dfCrime$wfir, 
#                                  dfCrime$wser, dfCrime$wmfg, dfCrime$wfed, dfCrime$wsta, 
#                                  dfCrime$wloc))
#names(dfEconVars) <- c('wcon', 'wtuc', 'wtrd', 'wfir', 'wser', 
#                              'wmfg', 'wfed', 'wsta', 'wloc')
#
#ggplot(melt(dfEconVars),aes(x=value)) + geom_histogram(bins=30) + facet_wrap(~variable)

#The economic variables
q1<-ggplot(data = dfCrime, aes(x = wcon, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q1a<-ggplot(data = dfCrime, aes(x = log(wcon), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q2<-ggplot(data = dfCrime, aes(x = wtuc, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q2a<-ggplot(data = dfCrime, aes(x = log(wtuc), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q3<-ggplot(data = dfCrime, aes(x = wtrd, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q3a<-ggplot(data = dfCrime, aes(x = log(wtrd), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q4<-ggplot(data = dfCrime, aes(x = wfir, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q4a<-ggplot(data = dfCrime, aes(x = log(wfir), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q5<-ggplot(data = dfCrime, aes(x = wser, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q5a<-ggplot(data = dfCrime, aes(x = log(wser), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q6<-ggplot(data = dfCrime, aes(x = wmfg, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q6a<-ggplot(data = dfCrime, aes(x = log(wmfg), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q7<-ggplot(data = dfCrime, aes(x = wfed, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q7a<-ggplot(data = dfCrime, aes(x = log(wfed), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q8<-ggplot(data = dfCrime, aes(x = wsta, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q8a<-ggplot(data = dfCrime, aes(x = log(wsta), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q9<-ggplot(data = dfCrime, aes(x = wloc, y = crmrte, color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")
q9a<-ggplot(data = dfCrime, aes(x = log(wloc), y = log(crmrte), color = metro)) + 
      geom_point()+
  geom_smooth(method = "lm")

options(repr.plot.width=8, repr.plot.height=16)
grid.arrange(q1, q1a, q2, q2a, q3, q3a, ncol=2)
grid.arrange(q4, q4a, q5, q5a, q6, q6a, ncol=2)
grid.arrange(q7, q7a, q8, q8a, q9, q9a, ncol=2)
```

The transforms make the relationship more linearly distributed.  We will transform these variables to their log equivalents.

```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logwcon<-log(dfCrime$wcon)
dfCrime$logwtuc<-log(dfCrime$wtuc)
dfCrime$logwtrd<-log(dfCrime$wtrd)
dfCrime$logwfir<-log(dfCrime$wfir)
dfCrime$logwser<-log(dfCrime$wser)
dfCrime$logwmfg<-log(dfCrime$wmfg)
dfCrime$logwfed<-log(dfCrime$wfed)
dfCrime$logwsta<-log(dfCrime$wsta)
dfCrime$logwloc<-log(dfCrime$wloc)
```

We move to the justice an law enforcement variables. With these variables being mostly < 1 we'll also take the log for comparison.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
#Plot of the criminal justice and law enforcment related variables vs crmrte
q1<-ggplot(data = dfCrime, aes(x = prbarr, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q1a<-ggplot(data = dfCrime, aes(x = log(prbarr), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q2<-ggplot(data = dfCrime, aes(x = prbconv, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q2a<-ggplot(data = dfCrime, aes(x = log(prbconv), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q3<-ggplot(data = dfCrime, aes(x = prbpris, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q3a<-ggplot(data = dfCrime, aes(x = log(prbpris), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q4<-ggplot(data = dfCrime, aes(x = avgsen, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q4a<-ggplot(data = dfCrime, aes(x = log(avgsen), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q5<-ggplot(data = dfCrime, aes(x = polpc, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q5a<-ggplot(data = dfCrime, aes(x = log(polpc), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q6<-ggplot(data = dfCrime, aes(x = mix, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q6a<-ggplot(data = dfCrime, aes(x = log(mix), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")

grid.arrange(q1, q1a, q2, q2a, q3, q3a, ncol=2)
grid.arrange(q4, q4a, q5, q5a, q6, q6a, ncol=2)
```

The log transformation for these variables makes the relationship more linear. We will transform these variables to their log equivalents.

We also note that of the six variables, only prbarr, prbconv and polpc show univariate correlation with crime. We believe these will be better candidates for our model selection. Further, we see mix has no correlation with crmrate and may be its own outcome variable.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logprbarr <- log(dfCrime$prbarr)
dfCrime$logprbconv <- log(dfCrime$prbconv)
dfCrime$logprbpris <- log(dfCrime$prbpris)
dfCrime$logavgsen <- log(dfCrime$avgsen)
dfCrime$logpolpc <- log(dfCrime$polpc)
dfCrime$logmix <- log(dfCrime$mix)
```

Next we take a look at the demographic variables and their log alternatives


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
q1<-ggplot(data = dfCrime, aes(x = pctymle, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q1a<-ggplot(data = dfCrime, aes(x = log(pctymle), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q2<-ggplot(data = dfCrime, aes(x = pctmin80, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q2a<-ggplot(data = dfCrime, aes(x = log(pctmin80), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q3<-ggplot(data = dfCrime, aes(x = density, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q3a<-ggplot(data = dfCrime, aes(x = log(density), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")


grid.arrange(q1, q1a, q2, q2a, q3, q3a, ncol=2)
```

Again we see improvements after transformation. We will include transforms of these variables as well.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logdensity <- log(dfCrime$density)
dfCrime$logpctmin80 <- log(dfCrime$pctmin80)
dfCrime$logpctymle <- log(dfCrime$pctymle)
```

Finally, we'll take a look at taxpc and a histogram of the crmrte variable itself.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
q1<-ggplot(data = dfCrime, aes(x = taxpc, y = crmrte, color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")
q1a<-ggplot(data = dfCrime, aes(x = log(taxpc), y = log(crmrte), color = regcode)) + 
      geom_point()+
  geom_smooth(method = "lm")

q2<-ggplot(data = dfCrime, aes(x = crmrte)) + 
      geom_histogram(bins=30)
q2a<-ggplot(data = dfCrime, aes(x = log(crmrte))) + 
      geom_histogram(bins=30)

grid.arrange(q1, q1a, q2, q2a, ncol=2)
```

The crmrte and taxpc variables also show improvement after transformation. We'll add those to our dataframe.


```{r, include=TRUE, comment=NA, warning=FALSE, echo=TRUE}
dfCrime$logcrmrte = log(dfCrime$crmrte)
dfCrime$logtaxpc = log(dfCrime$taxpc)
```
